{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":159685947,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/junhyeonkwon/deepfake-detection-revalid-lightning?scriptVersionId=171820440\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install lightning\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-04T12:37:00.718043Z","iopub.execute_input":"2024-04-04T12:37:00.7184Z","iopub.status.idle":"2024-04-04T12:37:30.605744Z","shell.execute_reply.started":"2024-04-04T12:37:00.718371Z","shell.execute_reply":"2024-04-04T12:37:30.60464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-04T12:37:30.607785Z","iopub.execute_input":"2024-04-04T12:37:30.608074Z","iopub.status.idle":"2024-04-04T12:37:34.473535Z","shell.execute_reply.started":"2024-04-04T12:37:30.608049Z","shell.execute_reply":"2024-04-04T12:37:34.472545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport math\nimport random\nfrom glob import glob\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, disable_beta_transforms_warning\nfrom torchvision.transforms import v2\nfrom torchvision.transforms import RandomErasing\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics.classification import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n\nfrom torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n\nimport lightning as L\nfrom lightning.pytorch import Trainer, seed_everything\nfrom lightning.pytorch.loggers import WandbLogger\n\ndisable_beta_transforms_warning()\n\nRANDOM_SEED = 42\nseed_everything(seed=RANDOM_SEED,workers=True)\n\n# Transforms ===============================================\n\nclass GuidedRandomErasing(RandomErasing):\n    \"\"\"Same as Random erasing but the rectangle region will appear with the\n    given normal distribution ~N(m,s). Pixel coordinates (i,j) will be scaled to \n    fit the x domain of range (-5,5) in the normal distribution curve\n    (x=(i-width/2)*10/width).\n    \n    m : (mean_y, mean_x)\n    s : (std_y, std_x)\n       \n    \"\"\"\n    def __init__(self, \n                 p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, \n                 m=(0.5,0.5), s=(1,1), inplace=False):\n        super().__init__(p,scale,ratio,value,inplace)\n        self.m = torch.tensor(m)\n        self.s = torch.tensor(s)\n        \n    @staticmethod \n    def get_erase_params(img, scale, ratio, mean, std, value=None):\n        img_c, img_h, img_w = img.shape[-3], img.shape[-2], img.shape[-1]\n        area = img_h * img_w\n        \n        log_ratio = torch.log(torch.tensor(ratio))\n        for _ in range(10):\n            erase_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n            aspect_ratio = torch.exp(torch.empty(1).uniform_(log_ratio[0], log_ratio[1])).item()\n\n            h = int(round(math.sqrt(erase_area * aspect_ratio)))\n            w = int(round(math.sqrt(erase_area / aspect_ratio)))\n            if not (h < img_h and w < img_w):\n                continue\n\n            # pick i, j\n            img_hw = torch.tensor((img_h,img_w))\n            i_j = torch.normal(mean=torch.zeros(2), std=std)*img_hw/10\n            i_j = i_j + mean*img_hw - torch.tensor((h,w))/2\n            i, j = int(i_j[0].item()), int(i_j[1].item())\n            # adjust h, w\n            if i < 0:\n                h -= -i\n                i = 0\n            elif i+h >= img_h:\n                h = img_h-i-1\n            if j < 0:\n                w -= -j\n                j = 0\n            elif j+w >= img_w:\n                w = img_w-j-1\n            # if h, w < 0 continue\n            if h <= 0 or w <= 0:\n                continue\n                \n            if value is None:\n                v = torch.empty([img_c, h, w], dtype=torch.float32).normal_()\n            else:\n                v = torch.tensor(value)[:, None, None]\n                \n                # print(f\"erase ratio {h*w/area}, {erase_area/area}\")\n            return i, j, h, w, v\n        \n        # Return original image\n        return 0, 0, img_h, img_w, img\n    \n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (Tensor): Tensor image to be erased.\n\n        Returns:\n            img (Tensor): Erased Tensor image.\n        \"\"\"\n        if torch.rand(1) < self.p:\n\n            # cast self.value to script acceptable type\n            if isinstance(self.value, (int, float)):\n                value = [float(self.value)]\n            elif isinstance(self.value, str):\n                value = None\n            elif isinstance(self.value, (list, tuple)):\n                value = [float(v) for v in self.value]\n            else:\n                value = self.value\n\n            if value is not None and not (len(value) in (1, img.shape[-3])):\n                raise ValueError(\n                    \"If value is a sequence, it should have either a single value or \"\n                    f\"{img.shape[-3]} (number of input channels)\"\n                )\n                \n            # TODO assert shape of mean and std\n            # I give up\n            x, y, h, w, v = GuidedRandomErasing.get_erase_params(\n                img, scale=self.scale, ratio=self.ratio,\n                mean = self.m, std = self.s, value=value)\n            return F.erase(img, x, y, h, w, v, self.inplace)\n        return img\n\n    def __repr__(self):\n        s0 = super().__repr__()\n        s = (\n            f\"{s0[:-1]}, \"\n            f\"mean={self.m} \"\n            f\"std={self.s})\")\n        return s\n    \n# transform loader\ndef get_valid_transform_RE(image_size, precision='32-true', random_erase=[]):\n    \"\"\"\n    random_erase List : list of dict with 'type' and 'kwargs' as keys\n    \"\"\"\n    # precision\n    if '16' in precision:\n        dtype = torch.float16\n    elif '32' in precision:\n        dtype = torch.float32\n    elif '64' in precision:\n        dtype = torch.float64\n    else:\n        raise NotImplementedError(f'{precision} not implemented')\n        \n    tr_list = [\n        v2.PILToTensor(),\n        v2.Resize((image_size, image_size),antialias=True)]\n    # random erase\n    for re_layer in random_erase:\n        if re_layer['type'].lower() == 'randomerasing':\n            tr_list.append(v2.RandomErasing(**re_layer['kwargs']))\n        elif re_layer['type'].lower() == 'guidedrandomerasing':\n            tr_list.append(GuidedRandomErasing(**re_layer['kwargs']))\n        else:\n            # warning\n            print(f\"{re_layer['type']} not implemented. Skipping {re_layer['type']}\")\n            continue\n    \n    valid_transform = v2.Compose([\n        *tr_list,\n        v2.ConvertImageDtype(dtype=dtype),\n        v2.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n    ])\n    return valid_transform\n\n# Datasets =======================================================\n\nclass MyDataset(Dataset):\n    def __init__(self, root, df, transform):\n        super(MyDataset).__init__()\n        self.root = root\n        self.paths = list(df['path'])\n        self.labels = list(df['label'])\n        self.transform = transform\n        \n    def __repr__(self):\n        out = super().__repr__()\n        return f\"{out} of size {self.__len__()}\\n{self.transform.__repr__()}\"\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        # get dtype conversion from the transform\n        dtype = torch.float32\n        for t in self.transform.transforms:\n            if isinstance(t,v2.ConvertImageDtype):\n                dtype = t.dtype\n        if self.labels[idx] == 'FAKE':\n            label = torch.zeros(1,dtype=dtype)\n        else:\n            label = torch.ones(1,dtype=dtype)\n        \n        img = Image.open(os.path.join(self.root,path))\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n\n# helper function =========================================================\n\n# model loader\ndef get_model_from_wandb(logger, artifact_name, device, save_dir_root='/kaggle/working'):\n    \"\"\"\n    model artifact will be downloaded as /{save_dir_root}/{run_id}/model.ckpt\n    \"\"\"\n    # download artifact to /kaggle/working\n    run_id = artifact_name.split('/')[2]\n    logger.download_artifact(artifact_name, save_dir=f\"{save_dir_root}/{run_id}\", artifact_type='model')\n    ckpt_path = sorted(glob(f\"{save_dir_root}/{run_id}/*.ckpt\"))[-1]\n    return LitEfficientNet.load_from_checkpoint(ckpt_path, map_location=device)\n\n# Lightning Module =========================================================\n\nclass LitEfficientNet(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = efficientnet_v2_s(weights = EfficientNet_V2_S_Weights)\n        self.model.classifier[1] = \\\n            nn.Linear(in_features=1280, out_features=1, bias=True)\n        \n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.acc_fn = BinaryAccuracy()\n        self.f1_score = BinaryF1Score() # ?\n        self.cm_fn = BinaryConfusionMatrix()\n        self.val_step_preds = []\n        self.val_step_ys = []\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.model(x)\n        loss = self.loss_fn(pred, y)\n        self.log('valid_loss', loss)\n        acc = self.acc_fn(pred, y)\n        self.log('valid_acc', acc)\n        f1_score = self.f1_score(pred, y)\n        self.log('valid_f1',f1_score)\n        self.val_step_preds.append(pred)\n        self.val_step_ys.append(y)\n        if torch.isnan(loss):\n            self.log(\"nan error\",1)\n        return loss\n    \n    def on_validation_epoch_end(self):\n        pred = torch.cat(self.val_step_preds)\n        y = torch.cat(self.val_step_ys)\n        self.log('total_f1', self.f1_score(pred, y))\n        self.log('total_acc', self.acc_fn(pred, y))\n        \n        y = y.flatten()\n        pred = pred.flatten()\n        \n        cm = self.cm_fn(pred, y)\n        for key, val in zip(['TN','FP','FN','TP'],cm.flatten().tolist()):\n            self.log(key,val)\n        \n        self.val_step_preds.clear()\n        self.val_step_ys.clear()\n            ","metadata":{"execution":{"iopub.status.busy":"2024-04-04T12:37:34.475199Z","iopub.execute_input":"2024-04-04T12:37:34.475766Z","iopub.status.idle":"2024-04-04T12:37:44.353371Z","shell.execute_reply.started":"2024-04-04T12:37:34.475734Z","shell.execute_reply":"2024-04-04T12:37:44.352578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read dataset metadata df\nFACE_DSET_META_PATH = '/kaggle/input/using-yunet/deepfake-detection-face-dataset.csv'\ndf = pd.read_csv(FACE_DSET_META_PATH)\ndf.head()\n# about the dataset\n# column name : video, frame_id, path, label, split\n# 73287 faces, 2448 videos, avg 29 faces per video\n# 82%(60343) Fakes, 18%(12944) Reals\n# 12 splits : fake > A1, A2, A3, ... ,E2 real > R1, R2\n\n# prepare dataframe in order to build dataset & dataloader\ndf_real = df[df['label'] == 'REAL']\ndf_real_vid = df_real['video'].drop_duplicates()\ndf_real_splits = []\nfor i in range(4):\n    drv = df_real_vid.sample(frac=1/(4-i), random_state=RANDOM_SEED)\n    df_real_vid.drop(drv.index, inplace = True)\n    df_real_splits.append(\n        pd.merge(drv,df_real,how='left',left_on='video',right_on='video'))\n    \ndf_fake = df[(df['label'] == 'FAKE') & (df['frame_id'] < 8)]\ndf_fake_vid = df_fake['video'].drop_duplicates()\ndf_fake_splits = []\nfor i in range(4):\n    dfv = df_fake_vid.sample(frac=1/(4-i),random_state=RANDOM_SEED)\n    df_fake_vid.drop(dfv.index, inplace=True)\n    df_fake_splits.append(\n        pd.merge(dfv,df_fake,how='left',left_on='video',right_on='video'))\n    \ndf_splits = [\n    pd.concat((df_fake_splits[i],df_real_splits[i])) \\\n    for i in range(4)\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-04T12:37:44.355283Z","iopub.execute_input":"2024-04-04T12:37:44.355739Z","iopub.status.idle":"2024-04-04T12:37:44.673212Z","shell.execute_reply.started":"2024-04-04T12:37:44.355713Z","shell.execute_reply":"2024-04-04T12:37:44.672117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.5^4 * (1*0 + 4*0.02 + 6*0.04 + 4*0.06 + 1*0.08) = 0.04\n# 0.25 * (1*0 + 2*a + 1*2*a) = a = 0.04\n\n# configs\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlogger_config = {\n    'logger_type' : 'wandblogger',\n    'project' : 'Deepfake_Detection-lightning-RE',\n    'log_model' : False,\n}\ndata_config = {\n    'root' : '/kaggle/input/using-yunet',\n    'image_size' : 224,\n    'batch_size' : 16,\n    'num_workers' : 3\n}\ntransform_layers = [\n    { # original random erasing 1\n        'type' : 'RandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.02,0.06),\n            'ratio' : (0.3,3.0),\n            'value' : 'random'\n        }\n    },\n    { # original random erasing 2\n        'type' : 'RandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.02,0.06),\n            'ratio' : (0.3,3.0),\n            'value' : 'random'\n        }\n    },\n    { # left eye random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.5,1.3),\n            'value' : 'random',\n            'm' : (0.41,0.61),\n            's' : (0.24,0.46)\n        }\n    },\n    { # right eye random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.5,1.3),\n            'value' : 'random',\n            'm' : (0.41,0.38),\n            's' : (0.24,0.43)\n        }\n    },\n    { # nose random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.3,1.2),\n            'value' : 'random',\n            'm' : (0.54,0.5),\n            's' : (0.24,0.7)\n        }\n    },\n    { # lips random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.2,1),\n            'value' : 'random',\n            'm' : (0.66,0.5),\n            's' : (0.22,0.48)\n        }\n    }\n]\nartifacts = [\n    # TODO copy paste artifact api ids\n    # a. gb0.1 - 이거 다시 돌려봐야함\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021253_fold_0:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021319_fold_1:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021345_fold_2:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021412_fold_3:v0',\n    # b. gb0.1-re0.2:1\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240728_fold_0:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240757_fold_1:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240826_fold_2:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240854_fold_3:v0',\n    # c. gb0.1-re0.2:2\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241057_fold_0:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241124_fold_1:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241150_fold_2:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241217_fold_3:v0'\n]\ntrained_group = [\n    'gb0.1', 'gb0.1-re0.2:1', 'gb0.1-re0.2:2'\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-04T12:37:44.674814Z","iopub.execute_input":"2024-04-04T12:37:44.675484Z","iopub.status.idle":"2024-04-04T12:37:44.709987Z","shell.execute_reply.started":"2024-04-04T12:37:44.675427Z","shell.execute_reply":"2024-04-04T12:37:44.709191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through given artifacts\nfor a_idx, artifact_id in enumerate(artifacts):\n    # download artifact if not exist at /kaggle/working\n    # init model\n    run_id = artifact_id.split('/')[-1]\n    fold_id = int(run_id.split(':')[0].split('_fold_')[-1])\n    if not os.path.exists(f'/kaggle/working/artifacts/{run_id}'):\n        model = get_model_from_wandb(\n            WandbLogger(), artifact_id, device, save_dir_root = '/kaggle/working/artifacts')\n    else:\n        print(f\"Artifact '{artifact_id}' already exists. Using existing file...\")\n        ckpt_path = sorted(glob(f\"/kaggle/working/artifacts/{run_id}/*.ckpt\"))[-1]\n        model = LitEfficientNet.load_from_checkpoint(ckpt_path, map_location=device)\n    \n    # iterate through given transform_layers\n    for t_id, t_layers in enumerate((transform_layers[0:2],transform_layers[2:6])):\n        # prep validation loader\n        valid_df = df_splits[fold_id]\n        valid_dset = MyDataset(data_config['root'], valid_df,\n                               get_valid_transform_RE(\n                                   image_size = data_config['image_size'],\n                                   random_erase = t_layers))\n        valid_loader = DataLoader(dataset=valid_dset, \n                                  batch_size=data_config['batch_size'],\n                                  num_workers=data_config['num_workers'],\n                                  drop_last=True)\n        print(valid_dset)\n        \n        # update logger exp config\n        # init logger\n        if logger_config['logger_type'].lower() == 'wandblogger':\n            logger = WandbLogger(\n                        project=logger_config['project'], \n                        name=f'fold_{fold_id}_t{t_id}',\n                        id=f\"{datetime.now().strftime('%y%m%d%H%M')}_f{fold_id}\",\n                        group=f\"{trained_group[a_idx//4]}_t{t_id}\", \n                        log_model=logger_config['log_model'])\n        else:\n            raise NotImplementedError('other logger types not implemented')\n        logger.experiment.config.update({\n            'Note' : artifact_id,\n            'logger_config' : logger_config,\n            'data_config' : data_config,\n            'transform_layers' : t_layers\n        })\n        # model.validate? (log with wandb)\n        trainer = Trainer(accelerator=device.type,logger=logger)\n        trainer.validate(model=model, dataloaders=valid_loader)\n        # wandb finish\n        if logger_config['logger_type'].lower() == 'wandblogger':\n            wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T08:56:46.500463Z","iopub.execute_input":"2024-04-03T08:56:46.500731Z"},"trusted":true},"execution_count":null,"outputs":[]}]}