{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":159685947,"sourceType":"kernelVersion"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/junhyeonkwon/deepfake-detection-lightning?scriptVersionId=171821718\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install lightning\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T07:08:15.298592Z","iopub.execute_input":"2024-04-03T07:08:15.298982Z","iopub.status.idle":"2024-04-03T07:08:45.145017Z","shell.execute_reply.started":"2024-04-03T07:08:15.298953Z","shell.execute_reply":"2024-04-03T07:08:45.143941Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.2.1-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2023.12.2)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.24.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.0.post0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nDownloading lightning-2.2.1-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.1\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T07:08:45.147233Z","iopub.execute_input":"2024-04-03T07:08:45.148017Z","iopub.status.idle":"2024-04-03T07:08:48.772088Z","shell.execute_reply.started":"2024-04-03T07:08:45.147979Z","shell.execute_reply":"2024-04-03T07:08:48.771213Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport random\nfrom glob import glob\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ExponentialLR, OneCycleLR\nfrom torchvision import datasets, disable_beta_transforms_warning\nfrom torchvision.transforms import v2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics.classification import BinaryAccuracy\n\nfrom torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n\nimport lightning as L\nfrom lightning.pytorch import Trainer, seed_everything\nfrom lightning.pytorch.loggers import CSVLogger, WandbLogger\nfrom lightning.pytorch.callbacks import LearningRateMonitor\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\ndisable_beta_transforms_warning()\n\nRANDOM_SEED = 42\nseed_everything(seed=RANDOM_SEED,workers=True)\n\n# Transforms ===============================================\n\ndef get_train_transform(\n    image_size, precision='32-true', \n    gaussian_blur = None, random_erase = None):\n    \n    if '16' in precision:\n        dtype = torch.float16\n    elif '32' in precision:\n        dtype = torch.float32\n    elif '64' in precision:\n        dtype = torch.float64\n    else:\n        raise NotImplementedError(f'{precision} not implemented')\n    tr_list = [\n        v2.PILToTensor(),\n        v2.Resize((image_size, image_size),antialias=True)]\n    if gaussian_blur:\n        tr_list.append(\n            v2.GaussianBlur(**gaussian_blur))\n    tr_list.append(\n        v2.RandomAdjustSharpness(sharpness_factor=2, p=0.5),)\n    if random_erase:\n        re = random_erase.copy()\n        n = re.pop('stack_layer',1)\n        for i in range(n):\n            tr_list.append(\n                v2.RandomErasing(**re))\n    \n    train_transform = v2.Compose([\n        *tr_list,\n        v2.RandomHorizontalFlip(p=0.5),\n        v2.ConvertImageDtype(dtype=dtype),\n        v2.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n    ])\n    return train_transform\n\ndef get_valid_transform(image_size, precision='32-true'):\n    if '16' in precision:\n        dtype = torch.float16\n    elif '32' in precision:\n        dtype = torch.float32\n    elif '64' in precision:\n        dtype = torch.float64\n    else:\n        raise NotImplementedError(f'{precision} not implemented')\n    \n    valid_transform = v2.Compose([\n        v2.PILToTensor(),\n        v2.Resize((image_size, image_size),antialias=True),\n        v2.ConvertImageDtype(dtype=dtype),\n        v2.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n    ])\n    return valid_transform\n\n# Datasets ===========================================================\n\nclass MyDataset(Dataset):\n    def __init__(self, root, df, transform):\n        super(MyDataset).__init__()\n        self.root = root\n        self.paths = list(df['path'])\n        self.labels = list(df['label'])\n        self.transform = transform\n        \n    def __repr__(self):\n        out = super().__repr__()\n        return f\"{out} of size {self.__len__()}\\n{self.transform.__repr__()}\"\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        # get dtype conversion from the transform\n        dtype = torch.float32\n        for t in self.transform.transforms:\n            if isinstance(t,v2.ConvertImageDtype):\n                dtype = t.dtype\n        if self.labels[idx] == 'FAKE':\n            label = torch.zeros(1,dtype=dtype)\n        else:\n            label = torch.ones(1,dtype=dtype)\n        \n        img = Image.open(os.path.join(self.root,path))\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n\n#===============================================\n# helper function\ndef get_ckpt_path(fold_id, wb_project=None):\n    if wb_project: # using wandb\n        root=f\"/kaggle/working/ckpts/{wb_project}/*_fold_{fold_id}\"\n        paths = glob(f\"{root}/checkpoints/*.ckpt\")\n    else: # using CSVLogger\n        root = f'/kaggle/working/ckpts/fold_{fold_id}'\n        paths = glob(f\"{root}/*/checkpoints/*.ckpt\")\n    if len(paths) > 0:\n        paths.sort()\n        return paths[-1]\n    else:\n        return None\n    \n#===============================================\n# Lightning Module\n\nclass LitEfficientNet(L.LightningModule):\n    def __init__(self, model_config):\n        '''\n        parameters:\n        model_config: model configurations required to initialize lightning model\n        '''\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = efficientnet_v2_s(weights = EfficientNet_V2_S_Weights)\n        self.model.classifier[1] = \\\n            nn.Linear(in_features=1280, out_features=1, bias=True)\n        # self.model.half\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.acc_fn = BinaryAccuracy() # TF threshold is 0.5\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.model(x)\n        loss = self.loss_fn(pred, y)\n        self.log(\"train_loss\",loss)\n        acc = self.acc_fn(pred, y)\n        self.log(\"train_acc\", acc)\n        if torch.isnan(loss):\n            self.log(\"nan error\", 1)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.model(x)\n        loss = self.loss_fn(pred, y)\n        self.log('valid_loss', loss)\n        acc = self.acc_fn(pred, y)\n        self.log(\"valid_acc\", acc)\n        if torch.isnan(loss):\n            self.log(\"nan error\", 1)\n        return loss\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        # get params from config dict\n        lr = self.hparams.model_config['learning_rate']\n        optim_cfg = self.hparams.model_config['optim_config']\n        sched_cfg = self.hparams.model_config['scheduler_config']\n        # set optimizer\n        if optim_cfg['optimizer'].lower() == 'sgd':\n            optimizer = optim.SGD(self.parameters(), \n                                  lr=lr, \n                                  momentum=optim_cfg['momentum'])\n        else:\n            raise NotImplementedError(\n                'optimizers other than SGD not implemented')\n        # set scheduler \n        if sched_cfg['scheduler'].lower() == 'onecyclelr':\n            scheduler = OneCycleLR(\n                optimizer,\n                max_lr=lr*sched_cfg['max_lr_coef'],\n                epochs=sched_cfg['epochs'],\n                steps_per_epoch=sched_cfg['steps_per_epoch'])\n        else:\n            raise NotImplementedError(\n                'schedulers other than onecyclelr not implemented')\n            \n        return {\n            'optimizer':optimizer,\n            'lr_scheduler':{\n                \"scheduler\": scheduler,\n                \"interval\" : sched_cfg['interval']\n            }\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-03T07:08:48.780122Z","iopub.execute_input":"2024-04-03T07:08:48.780392Z","iopub.status.idle":"2024-04-03T07:08:56.938844Z","shell.execute_reply.started":"2024-04-03T07:08:48.78037Z","shell.execute_reply":"2024-04-03T07:08:56.938063Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"}]},{"cell_type":"code","source":"# read dataset metadata df\nFACE_DSET_META_PATH = '/kaggle/input/using-yunet/deepfake-detection-face-dataset.csv'\ndf = pd.read_csv(FACE_DSET_META_PATH)\ndf.head()\n# about the dataset\n# column name : video, frame_id, path, label, split\n# 73287 faces, 2448 videos, avg 29 faces per video\n# 82%(60343) Fakes, 18%(12944) Reals\n# 12 splits : fake > A1, A2, A3, ... ,E2 real > R1, R2","metadata":{"execution":{"iopub.status.busy":"2024-04-03T07:08:56.9399Z","iopub.execute_input":"2024-04-03T07:08:56.940345Z","iopub.status.idle":"2024-04-03T07:08:57.165274Z","shell.execute_reply.started":"2024-04-03T07:08:56.940306Z","shell.execute_reply":"2024-04-03T07:08:57.164359Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"            video  frame_id  \\\n0  aodrcrvodk.mp4         0   \n1  aodrcrvodk.mp4         1   \n2  aodrcrvodk.mp4         2   \n3  aodrcrvodk.mp4         3   \n4  aodrcrvodk.mp4         4   \n\n                                                path label split  \n0  deepfake-detection-face-dataset/aodrcrvodk/00.jpg  FAKE    A1  \n1  deepfake-detection-face-dataset/aodrcrvodk/01.jpg  FAKE    A1  \n2  deepfake-detection-face-dataset/aodrcrvodk/02.jpg  FAKE    A1  \n3  deepfake-detection-face-dataset/aodrcrvodk/03.jpg  FAKE    A1  \n4  deepfake-detection-face-dataset/aodrcrvodk/04.jpg  FAKE    A1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video</th>\n      <th>frame_id</th>\n      <th>path</th>\n      <th>label</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aodrcrvodk.mp4</td>\n      <td>0</td>\n      <td>deepfake-detection-face-dataset/aodrcrvodk/00.jpg</td>\n      <td>FAKE</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aodrcrvodk.mp4</td>\n      <td>1</td>\n      <td>deepfake-detection-face-dataset/aodrcrvodk/01.jpg</td>\n      <td>FAKE</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aodrcrvodk.mp4</td>\n      <td>2</td>\n      <td>deepfake-detection-face-dataset/aodrcrvodk/02.jpg</td>\n      <td>FAKE</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aodrcrvodk.mp4</td>\n      <td>3</td>\n      <td>deepfake-detection-face-dataset/aodrcrvodk/03.jpg</td>\n      <td>FAKE</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aodrcrvodk.mp4</td>\n      <td>4</td>\n      <td>deepfake-detection-face-dataset/aodrcrvodk/04.jpg</td>\n      <td>FAKE</td>\n      <td>A1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Experiment 1\n# 15K Fakes + 13K Reals --> 4 Fold CV\n\n# hyperparameters\nlimit_fold = 4\ntrainer_config = {\n    'accelerator' : 'auto',\n    'precision' : '16-true',\n    'max_epochs' : 6,\n    'limit_train_batches' : 1.0,\n    'limit_val_batches' : 1.0,\n    'log_every_n_steps' : 32,\n    'profiler' : None,\n    'num_sanity_val_steps' : -1,\n    'accumulate_grad_batches' : 4,\n    'detect_anomaly'  :  False,\n    'check_val_every_n_epoch' : 1,\n    'deterministic' : True,\n    'enable_checkpointing' : True,\n    'gradient_clip_val' : 2\n}\ncallbacks_config = {\n    'lr_monitor' : {\n        'logging_interval' : 'step'\n    }, \n    'early_stopping' : {\n        'monitor' : 'valid_loss',\n        'min_delta' : 0.0,\n        'patience' : 3\n    }\n}\nlogger_config = {\n    'logger_type' : 'wandblogger',\n    'project' : 'Deepfake_Detection-lightning-4cv',\n    'log_model' : True,\n    'group' : 'exp2-gblur_s0.1'\n}\ndata_config = {\n    'root' : '/kaggle/input/using-yunet',\n    'image_size' : 224,\n    'gaussian_blur' : {\n        'kernel_size' : (5,5),\n        'sigma' : 0.1\n    },\n    'random_erase' : None,\n    'batch_size' : 16,\n    'num_workers' : 3\n}\nmodel_config = {\n    'learning_rate' : 1e-4,\n    'optim_config' : {\n        'optimizer' : 'SGD',\n        'momentum' : .9,\n        'weight_decay' : 1.2e-3\n    },\n    'scheduler_config' : {\n        'scheduler' : 'OneCycleLR',\n        'max_lr_coef' : 50,\n        'interval' : 'step'\n    }\n} \n\n# prepare dataframe in order to build dataset & dataloader\ndf_real = df[df['label'] == 'REAL']\ndf_real_vid = df_real['video'].drop_duplicates()\ndf_real_splits = []\nfor i in range(4):\n    drv = df_real_vid.sample(frac=1/(4-i), random_state=RANDOM_SEED)\n    df_real_vid.drop(drv.index, inplace = True)\n    df_real_splits.append(\n        pd.merge(drv,df_real,how='left',left_on='video',right_on='video'))\n    \ndf_fake = df[(df['label'] == 'FAKE') & (df['frame_id'] < 8)]\ndf_fake_vid = df_fake['video'].drop_duplicates()\ndf_fake_splits = []\nfor i in range(4):\n    dfv = df_fake_vid.sample(frac=1/(4-i),random_state=RANDOM_SEED)\n    df_fake_vid.drop(dfv.index, inplace=True)\n    df_fake_splits.append(\n        pd.merge(dfv,df_fake,how='left',left_on='video',right_on='video'))\n    \ndf_splits = [\n    pd.concat((df_fake_splits[i],df_real_splits[i])) \\\n    for i in range(4)\n]\n\n# 4 Fold CV Training ===========================\nfor fold_id in range(min(limit_fold,4)):\n    # prepare train loader\n    train_df = pd.concat([df_splits[i] for i in range(4) if i != fold_id])\n    train_dset = MyDataset(data_config['root'],train_df,\n                           get_train_transform(\n                               image_size = data_config['image_size'],\n                               precision = trainer_config['precision'],\n                               gaussian_blur=data_config['gaussian_blur'],\n                               random_erase=data_config['random_erase']))\n    train_loader = DataLoader(dataset=train_dset, \n                              batch_size=data_config['batch_size'],\n                              shuffle=True,\n                              num_workers=data_config['num_workers'])\n    # prepare validation loader\n    valid_df = df_splits[fold_id]\n    valid_dset = MyDataset(data_config['root'],valid_df,\n                          get_valid_transform(\n                              image_size = data_config['image_size'],\n                              precision = trainer_config['precision']))\n    valid_loader = DataLoader(dataset=valid_dset, \n                              batch_size=data_config['batch_size'], \n                              num_workers=data_config['num_workers'],\n                              drop_last=True)\n    print(train_dset, valid_dset)\n    \n    # init model, logger, monitor, trainer\n    if model_config['scheduler_config']['scheduler'].lower() == 'onecyclelr':\n        model_config['scheduler_config']['epochs'] = \\\n            trainer_config['max_epochs']\n        lim_tb = trainer_config['limit_train_batches']\n        acc_gb = trainer_config['accumulate_grad_batches']\n        model_config['scheduler_config']['steps_per_epoch'] = \\\n            int(len(train_loader)*lim_tb/acc_gb)+1\n    \n    model = LitEfficientNet(model_config = model_config)\n    ## use wandb logger when specified\n    if logger_config['logger_type'].lower() == 'wandblogger':\n        logger = WandbLogger(\n            project=logger_config['project'],\n            name=f\"fold_{fold_id}\",\n            id=f\"{datetime.now().strftime('%y%m%d%H%M')}_fold_{fold_id}\",\n            group=logger_config['group'],\n            log_model=logger_config['log_model'],\n            save_dir=f'/kaggle/working/ckpts')\n        logger.experiment.config.update({\n            'limit_fold' : limit_fold,\n            'trainer_config': trainer_config,\n            'callbacks_config':callbacks_config,\n            'logger_config' : logger_config,\n            'data_config' : data_config,\n            'model_config' : model_config\n        })\n    else: # use CSV logger as default\n        logger = CSVLogger(name=f\"fold_{fold_id}\",\n                           save_dir=f'/kaggle/working/ckpts')\n        logger.log_hyperparams({\n            'limit_fold' : limit_fold,\n            'trainer_config': trainer_config,\n            'callbacks_config':callbacks_config,\n            'logger_config' : logger_config,\n            'data_config' : data_config,\n            'model_config' : model_config\n        })\n    # init lr monitor\n    callbacks = []\n    if callbacks_config.get('lr_monitor',False):\n        callbacks.append(\n            LearningRateMonitor(**callbacks_config['lr_monitor']))\n    # init early stopping callback\n    if callbacks_config.get('early_stopping',False):\n        callbacks.append(\n            EarlyStopping(**callbacks_config['early_stopping']))\n    # init trainer\n    trainer = Trainer(**trainer_config, \n                      default_root_dir=f'/kaggle/working/ckpts',\n                      logger=logger,\n                      callbacks=callbacks)\n\n    # run training\n    trainer.fit(model=model, \n                train_dataloaders=train_loader, \n                val_dataloaders=valid_loader)\n    # wandb finish\n    if logger_config['logger_type'].lower() == 'wandblogger':\n        wandb.finish()\n    ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-03T07:08:57.166747Z","iopub.execute_input":"2024-04-03T07:08:57.167014Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"<__main__.MyDataset object at 0x78a27aac64a0> of size 21482\nCompose(\n      PILToTensor()\n      Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n      GaussianBlur(kernel_size=(5, 5), sigma=[0.1, 0.1])\n      RandomAdjustSharpness(p=0.5, sharpness_factor=2)\n      RandomHorizontalFlip(p=0.5)\n      ConvertImageDtype()\n      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=False)\n) <__main__.MyDataset object at 0x78a27aac65f0> of size 7098\nCompose(\n      PILToTensor()\n      Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n      ConvertImageDtype()\n      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=False)\n)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 158MB/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluanakwon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/working/ckpts/wandb/ wasn't writable, using system temp directory.\nwandb: WARNING Path /kaggle/working/ckpts/wandb/ wasn't writable, using system temp directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/tmp/wandb/run-20240403_070858-2404030708_fold_0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030708_fold_0' target=\"_blank\">fold_0</a></strong> to <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv' target=\"_blank\">https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030708_fold_0' target=\"_blank\">https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030708_fold_0</a>"},"metadata":{}},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\nINFO: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name    | Type              | Params\n----------------------------------------------\n0 | model   | EfficientNet      | 20.2 M\n1 | loss_fn | BCEWithLogitsLoss | 0     \n2 | acc_fn  | BinaryAccuracy    | 0     \n----------------------------------------------\n20.2 M    Trainable params\n0         Non-trainable params\n20.2 M    Total params\n80.715    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1a4d69b9fdc4ac5a08d574b7e75509f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁</td></tr><tr><td>lr-SGD</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▅▅▅▅████</td></tr><tr><td>train_acc</td><td>▁█▁▁▃</td></tr><tr><td>train_loss</td><td>█▁▇▆▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-SGD</td><td>0.00098</td></tr><tr><td>train_acc</td><td>0.625</td></tr><tr><td>train_loss</td><td>0.66846</td></tr><tr><td>trainer/global_step</td><td>159</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fold_0</strong> at: <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030708_fold_0' target=\"_blank\">https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030708_fold_0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/tmp/wandb/run-20240403_070858-2404030708_fold_0/logs</code>"},"metadata":{}},{"name":"stdout","text":"<__main__.MyDataset object at 0x78a1ad45d300> of size 21405\nCompose(\n      PILToTensor()\n      Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n      GaussianBlur(kernel_size=(5, 5), sigma=[0.1, 0.1])\n      RandomAdjustSharpness(p=0.5, sharpness_factor=2)\n      RandomHorizontalFlip(p=0.5)\n      ConvertImageDtype()\n      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=False)\n) <__main__.MyDataset object at 0x78a27aac6380> of size 7175\nCompose(\n      PILToTensor()\n      Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n      ConvertImageDtype()\n      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=False)\n)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/tmp/wandb/run-20240403_071234-2404030712_fold_1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030712_fold_1' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv' target=\"_blank\">https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030712_fold_1' target=\"_blank\">https://wandb.ai/luanakwon/Deepfake_Detection-lightning-4cv/runs/2404030712_fold_1</a>"},"metadata":{}},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\nINFO: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name    | Type              | Params\n----------------------------------------------\n0 | model   | EfficientNet      | 20.2 M\n1 | loss_fn | BCEWithLogitsLoss | 0     \n2 | acc_fn  | BinaryAccuracy    | 0     \n----------------------------------------------\n20.2 M    Trainable params\n0         Non-trainable params\n20.2 M    Total params\n80.715    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c6130ac78e04facaea52f8f473e1a5b"}},"metadata":{}}]},{"cell_type":"code","source":"# simple plot in the case CSVLogger\nif logger_config['logger_type'].lower() != 'wandblogger':\n    log_path = glob('/kaggle/working/ckpts/fold_0/version_*/metrics.csv')\n    log_path.sort()\n    print(log_path)\n    log_df = pd.read_csv(log_path[-1])\n    log_df.head()\n\n    plt.figure(figsize=(5,10))\n    for i, c in enumerate(log_df.columns):\n        plt.subplot(len(log_df.columns),1,i+1)\n        plt.ylabel(c)\n        plt.plot(log_df['step'],log_df[c],'b.',\n                 label=f\"last {c} {log_df[c].dropna().iloc[-1]}\")\n        plt.legend()\n    plt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## visualize validation result\n        \n    \n# dataset class with filepath included\nclass MyDatasetwithFilepath(MyDataset):    \n    def __getitem__(self, idx):\n        img, label = super().__getitem__(idx)\n        path = os.path.join(self.root, self.paths[idx])\n        return img, label, path\n    \n######## pick the worst 10 validation image ############\nn_samples = 6\nfig = plt.figure(figsize=(n_samples,min(limit_fold,4)))\nfor fold_id in range(min(limit_fold,4)):\n    # prepare validation loader\n    valid_df = df_splits[fold_id]\n    valid_dset = MyDatasetwithFilepath(data_config['root'],valid_df,\n                          get_valid_transform(\n                              image_size = data_config['image_size']))\n    valid_loader = DataLoader(dataset=valid_dset, \n                              batch_size=data_config['batch_size'], \n                              num_workers=data_config['num_workers'],\n                              drop_last=True)\n    \n    # init model, loss fn, results dict\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    if logger_config['logger_type'].lower() == 'wandblogger':\n        ckpt_path = get_ckpt_path(fold_id, logger_config['project'])\n    else:\n        ckpt_path = get_ckpt_path(fold_id)\n        \n    if ckpt_path is not None:\n        model = LitEfficientNet.load_from_checkpoint(ckpt_path, \n                                                 map_location=device)\n    else:\n        print(f\"No checkpoints found in fold_{fold_id}. skipping fold_{fold_id}.\")\n        continue\n    sigmoid_fn = nn.Sigmoid()\n    loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n    validation_result = {'path':[],'pred':[], 'target':[], 'loss':[]}\n    print(f\"model loaded from {ckpt_path}\")\n    \n    # run validation\n    model.eval()\n    with torch.no_grad():\n        for x, y, paths in tqdm(valid_loader):\n            x = x.to(device)\n            y = y.to(device)\n            pred = model.forward(x)\n            loss = loss_fn(pred, y)\n            pred = sigmoid_fn(pred)\n            validation_result['path'] += paths\n            validation_result['pred'].append(pred)\n            validation_result['target'].append(y)\n            validation_result['loss'].append(loss)\n        \n    for key in ['pred','target','loss']:\n        validation_result[key] = \\\n            torch.concat(validation_result[key]).cpu().flatten()\n    validated_df = pd.DataFrame(validation_result)\n    validated_df = validated_df.sort_values(by='loss',ascending=False).head(n_samples)\n    for i, (_, row) in enumerate(validated_df.iterrows()):\n        print(row['path'])\n        image = Image.open(row['path'])\n        ax = fig.add_subplot(min(limit_fold,4),n_samples,i+fold_id*n_samples+1)\n        ax.imshow(image)\n        ax.axis('off')\n        ax.set_title(f\"{row['target']}, {row['pred']:.6f}\",fontsize=7)\n\nplt.show()\nfig.savefig('/kaggle/working/bad_samples.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weird things\nTraining with SGD when transfer learning / finetuning is a convention, snd I followed it. At first it sort of showed a decrease in loss metric, but it was not the best graph. Tried different LR scheduler and initial LRs, but the impact was hardly recognizable. Also, small LRs like 1e-5 gave a stable graph, but the convergence was too slow and I personally hardly ever saw using LR that small. Hence, tried starting with initial LR of 1e-4 with hard lr decay, and got some nan loss value in validation phase. The next approach I tried was using Adam, hoping nans and slow convergence might be solved. but Adam made all of the model weights to nan in single step, even with super small lr. Did not go any further with Adam but while debugging I've found that pretrained weight does not output nan but at the first epoch, even with lr=5e-5 + SGD, some weight becomes nan. But then on the following epoch this disappears, even with the same lr. My explanation of this, though I did not go very deep, is that fine-tuned pretrained weight is very vulnerable to gradient exploding and in order to avoid such, I tried OneCycleLR which starts small, gets bigger, and ends very small.\nThe other thing to note is the impact of batch size. First thing, which is obvious, having same epoch and bigger batch size slows down training. Bigger batch size might direct the gradient better towards the optimal minima, but with same LR, there will be lesser steps per epoch thus slowed down. Second one idk... \nPoint is, Batch size is not just about hardware limit or training speed, but it has significant impact on final performance of the model. More experiment and explanation from other AI learner here(KR) -> https://inhovation97.tistory.com/32\n\n\n2. once the seed is set, Dataframe.sample() on same dataframe returns the same, no matter how many times called.","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nfrom torchvision.transforms import v2\nfrom PIL import Image\n\n\nfrom torchvision.transforms import functional as F\n\nclass GuidedRandomErasing(v2.RandomErasing):\n    \"\"\"Same as Random erasing but the rectangle region will appear with the\n    given normal distribution ~N(m,s). Pixel coordinates (i,j) will be scaled to \n    fit the x domain of range (-5,5) in the normal distribution curve\n    (x=(i-width/2)*10/width).\n    \n    m : (mean_y, mean_x)\n    s : (std_y, std_x)\n       \n    \"\"\"\n    def __init__(self, \n                 p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, \n                 m=(0.5,0.5), s=(1,1), inplace=False):\n        super().__init__(p,scale,ratio,value,inplace)\n        self.m = torch.tensor(m)\n        self.s = torch.tensor(s)\n        \n    @staticmethod \n    def get_erase_params(img, scale, ratio, mean, std, value=None):\n        img_c, img_h, img_w = img.shape[-3], img.shape[-2], img.shape[-1]\n        area = img_h * img_w\n        \n        log_ratio = torch.log(torch.tensor(ratio))\n        for _ in range(10):\n            erase_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n            aspect_ratio = torch.exp(torch.empty(1).uniform_(log_ratio[0], log_ratio[1])).item()\n\n            h = int(round(math.sqrt(erase_area * aspect_ratio)))\n            w = int(round(math.sqrt(erase_area / aspect_ratio)))\n            if not (h < img_h and w < img_w):\n                continue\n\n            # pick i, j\n            img_hw = torch.tensor((img_h,img_w))\n            i_j = torch.normal(mean=torch.zeros(2), std=std)*img_hw/10\n            i_j = i_j + mean*img_hw - torch.tensor((h,w))\n            i, j = int(i_j[0].item()), int(i_j[1].item())\n            # adjust h, w\n            if i < 0:\n                h -= -i\n                i = 0\n            elif i+h >= img_h:\n                h = img_h-i-1\n            if j < 0:\n                w -= j\n                j = 0\n            elif j+w >= img_w:\n                w = img_w-j-1\n            # if h, w < 0 continue\n            if h <= 0 or w <= 0:\n                continue\n                \n            if value is None:\n                v = torch.empty([img_c, h, w], dtype=torch.float32).normal_()\n            else:\n                v = torch.tensor(value)[:, None, None]\n                \n                # print(f\"erase ratio {h*w/area}, {erase_area/area}\")\n            return i, j, h, w, v\n        \n        # Return original image\n        return 0, 0, img_h, img_w, img\n    \n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (Tensor): Tensor image to be erased.\n\n        Returns:\n            img (Tensor): Erased Tensor image.\n        \"\"\"\n        if torch.rand(1) < self.p:\n\n            # cast self.value to script acceptable type\n            if isinstance(self.value, (int, float)):\n                value = [float(self.value)]\n            elif isinstance(self.value, str):\n                value = None\n            elif isinstance(self.value, (list, tuple)):\n                value = [float(v) for v in self.value]\n            else:\n                value = self.value\n\n            if value is not None and not (len(value) in (1, img.shape[-3])):\n                raise ValueError(\n                    \"If value is a sequence, it should have either a single value or \"\n                    f\"{img.shape[-3]} (number of input channels)\"\n                )\n                \n            # TODO assert shape of mean and std\n            # I give up\n            x, y, h, w, v = GuidedRandomErasing.get_erase_params(\n                img, scale=self.scale, ratio=self.ratio,\n                mean = self.m, std = self.s, value=value)\n            return F.erase(img, x, y, h, w, v, self.inplace)\n        return img\n\n    def __repr__(self):\n        s0 = super().__repr__()\n        s = (\n            f\"{s0[:-1]}, \"\n            f\"mean={self.m} \"\n            f\"std={self.s})\")\n        return s\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\nGRE = GuidedRandomErasing(p=0.5,scale=(0.01,0.03),ratio=(0.3,1.3),m=(0.41,0.38),s=(0.24,0.48))\n\nblank = torch.zeros((1, 224,224))\n\nfor _ in range(10):\n    img = torch.ones((1, 224,224))\n    img = GRE(img)\n    #print(torch.min(img), torch.max(img))\n    blank += img\n    \nblank /= torch.max(blank)+1\nplt.imshow(blank.view(224,224))\nplt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation code\n# transforms are\n# 1. random erase * 2\n# 2. guided random erase on eyes + nose + lips with uniform probability\n#   - both transform have same expected_area_to_be_erased\n# trained versions are\n# a. gb0.1\n# b. gb0.1-re0.2:1\n# c. gb0.1-re0.2:2\n# total 6 validations\n\n# If Possible : using the best version, run eyes/nose/lips RE validations seperately \n# to see if certain part has more critical info.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform loader\ndef get_valid_transform_RE(image_size, precision='32-true', random_erase=[]):\n    \"\"\"\n    random_erase List : list of dict with 'type' and 'kwargs' as keys\n    \"\"\"\n    # precision\n    if '16' in precision:\n        dtype = torch.float16\n    elif '32' in precision:\n        dtype = torch.float32\n    elif '64' in precision:\n        dtype = torch.float64\n    else:\n        raise NotImplementedError(f'{precision} not implemented')\n        \n    tr_list = [\n        v2.PILToTensor(),\n        v2.Resize((image_size, image_size),antialias=True)]\n    # random erase\n    for re_layer in random_erase:\n        if re_layer['type'].lower() == 'randomerasing':\n            tr_list.append(v2.RandomErasing(**re_layer['kwargs']))\n        elif re_layer['type'].lower() == 'guidedrandomerasing':\n            tr_list.append(GuidedRandomErasing(**re_layer['kwargs']))\n        else:\n            # warning\n            print(f\"{re_layer['type']} not implemented. Skipping {re_layer['type']}\")\n            continue\n    \n    valid_transform = v2.Compose([\n        *tr_list,\n        v2.ConvertImageDtype(dtype=dtype),\n        v2.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n    ])\n    return valid_transform\n    \n\n# model loader\ndef get_model_from_wandb(logger, artifact_name, device, save_dir='/kaggle/working'):\n    # download artifact to /kaggle/working\n    run_id = artifact_name.split('/')[2]\n    logger.download_artifact(artifact_name, save_dir=save_dir, artifact_type='model')\n    ckpt_path = sorted(glob(f\"{save_dir}/artifacts/{run_id}/*.ckpt\"))[-1]\n    return LitEfficientNet.load_from_checkpoint(ckpt_path, map_location=device)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.5^4 * (1*0 + 4*0.02 + 6*0.04 + 4*0.06 + 1*0.08) = 0.04\n# 0.25 * (1*0 + 2*a + 1*2*a) = a = 0.04\n\n# configs\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlogger_config = {\n    'logger_type' : 'wandblogger',\n    'project' : 'Deepfake_Detection-lightning-4cv',\n    'log_model' : False,\n    'group' : 're_validations'\n}\ndata_config = {\n    'root' : '/kaggle/working/input/using-yunet',\n    'image_size' : 224,\n    'batch_size' : 16,\n    'num_workers' : 3\n}\ntransform_layers = [\n    { # original random erasing 1\n        'type' : 'RandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.02,0.06),\n            'ratio' : (0.3,3.0),\n            'value' : 'random'\n        }\n    },\n    { # original random erasing 2\n        'type' : 'RandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.02,0.06),\n            'ratio' : (0.3,3.0),\n            'value' : 'random'\n        }\n    },\n    { # left eye random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.5,1.3),\n            'value' : 'random',\n            'm' : (0.41,0.61),\n            's' : (0.24,0.46)\n        }\n    },\n    { # right eye random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.5,1.3),\n            'value' : 'random',\n            'm' : (0.41,0.38),\n            's' : (0.24,0.43)\n        }\n    },\n    { # nose random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.3,1.2),\n            'value' : 'random',\n            'm' : (0.54,0.5),\n            's' : (0.24,0.7)\n        }\n    },\n    { # lips random erasing\n        'type' : 'GuidedRandomErasing',\n        'kwargs' : {\n            'p' : 0.5,\n            'scale' : (0.01,0.03),\n            'ratio' : (0.2,1),\n            'value' : 'random',\n            'm' : (0.66,0.5),\n            's' : (0.22,0.48)\n        }\n    }\n]\nartifacts = [\n    # TODO copy paste artifact api ids\n    # a. gb0.1 - 이거 다시 돌려봐야함\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021253_fold_0:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021319_fold_1:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021345_fold_2:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2404021412_fold_3:v0',\n    # b. gb0.1-re0.2:1\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240728_fold_0:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240757_fold_1:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240826_fold_2:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403240854_fold_3:v0',\n    # c. gb0.1-re0.2:2\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241057_fold_0:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241124_fold_1:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241150_fold_2:v0',\n    'luanakwon/Deepfake_Detection-lightning-4cv/model-2403241217_fold_3:v0'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=RANDOM_SEED, workers=True)\n# re-initialize dataframe\nFACE_DSET_META_PATH = '/kaggle/input/using-yunet/deepfake-detection-face-dataset.csv'\ndf = pd.read_csv(FACE_DSET_META_PATH)\n# prepare dataframe in order to build dataset & dataloader\ndf_real = df[df['label'] == 'REAL']\ndf_real_vid = df_real['video'].drop_duplicates()\ndf_real_splits = []\nfor i in range(4):\n    drv = df_real_vid.sample(frac=1/(4-i), random_state=RANDOM_SEED)\n    df_real_vid.drop(drv.index, inplace = True)\n    df_real_splits.append(\n        pd.merge(drv,df_real,how='left',left_on='video',right_on='video'))\n    \ndf_fake = df[(df['label'] == 'FAKE') & (df['frame_id'] < 8)]\ndf_fake_vid = df_fake['video'].drop_duplicates()\ndf_fake_splits = []\nfor i in range(4):\n    dfv = df_fake_vid.sample(frac=1/(4-i),random_state=RANDOM_SEED)\n    df_fake_vid.drop(dfv.index, inplace=True)\n    df_fake_splits.append(\n        pd.merge(dfv,df_fake,how='left',left_on='video',right_on='video'))\n    \ndf_splits = [\n    pd.concat((df_fake_splits[i],df_real_splits[i])) \\\n    for i in range(4)\n]\n# init logger\nif logger_config['logger_type'].lower() == 'wandblogger':\n    logger = WandbLogger()\nelse:\n    raise NotImplementedError('other logger types not implemented')\n\n# iterate through given artifacts\nfor artifact_id in artifacts:\n    # download artifact if not exist at /kaggle/working\n    # init model\n    run_id = artifact_id.split('/')[-1]\n    fold_id = int(run_id.split(':')[0].split('_fold_')[-1])\n    if not os.path.exists(f'/kaggle/working/artifacts/{run_id}'):\n        model = get_model_from_wandb(logger, artifact_id, device)\n    else:\n        print(f\"Artifact '{artifact}' already exists. Using existing file...\")\n        ckpt_path = sorted(glob(f\"/kaggle/working/artifacts/{run_id}/*.ckpt\"))[-1]\n        model = LitEfficientNet.load_from_checkpoint(ckpt_path, map_location=device)\n    \n    # iterate through given transform_layers\n    for t_layers in (transform_layers[0:1],transform_layers[2:6]):\n        # prep validation loader\n        valid_df = df_splits[fold_id]\n        valid_dset = MyDataset(data_config['root'], valid_df,\n                               get_valid_transform_RE(\n                                   image_size = data_config['image_size'],\n                                   random_erase = t_layers))\n        valid_loader = DataLoader(dataset=valid_dset, \n                                  batch_size=data_config['batch_size'],\n                                  num_workers=data_config['num_workers'],\n                                  drop_last=True)\n        print(valid_dset)\n        \n        # update logger exp config\n        logger.experiment.config.update({\n            'Note' : artifact_id,\n            'logger_config' : logger_config,\n            'data_config' : data_config,\n            'transform_layers' : t_layers\n        })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zhong, Z., Zheng, L., Kang, G., Li, S., & Yang, Y. (2020). Random Erasing Data Augmentation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07), 13001-13008. https://doi.org/10.1609/aaai.v34i07.7000\n\nLewy, Dominik & Mańdziuk, Jacek. (2021). An overview of mixing augmentation methods and augmentation strategies. \n\nHaliassos, A., Vougioukas, K., Petridis, S., & Pantic, M. (2020). Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery Detection. ArXiv. /abs/2012.07657\n","metadata":{}}]}